# app.py
import sys
import io
import os

# --- ã€é‡è¦ã€‘Streamlit Cloudç”¨ SQLiteå¯¾ç­– ---
# Streamlit Cloudã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆSQLiteã¯å¤ã„ãŸã‚ã€pysqlite3ã‚’ä½¿ã£ã¦æ–°ã—ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«ç½®ãæ›ãˆã¾ã™
try:
    __import__('pysqlite3')
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
except ImportError:
    pass # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒãªã©pysqlite3ãŒãªã„å ´åˆã¯ä½•ã‚‚ã—ãªã„
# -------------------------------------------

import pandas as pd
import streamlit as st
from typing import Annotated, TypedDict

# LangGraph & LangChain Imports
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from langgraph.graph.message import add_messages
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
import httpx # ngrokãƒ˜ãƒƒãƒ€ãƒ¼ç”¨

# RAGç”¨ Imports
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

# -----------------------------------------------------------------
# â–¼ è¨­å®šãƒ»å®šæ•°
# -----------------------------------------------------------------
st.set_page_config(page_title="Legal Analysis Chatbot", layout="wide")
st.title("ğŸ¤– è¦ªæ¨©å–ªå¤±ãƒ»åœæ­¢äº‹ä¾‹åˆ†æãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ (Team Demo)")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
with st.sidebar:
    st.header("ğŸ” æ¥ç¶šè¨­å®š")
    
    # 1. ãƒãƒ¼ãƒ èªè¨¼ (ç°¡æ˜“ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰)
    team_password = st.secrets.get("auth", {}).get("team_password", "demo1234")
    input_password = st.text_input("Team Password", type="password")
    is_authenticated = input_password == team_password

    if is_authenticated:
        st.success("èªè¨¼OK")
    else:
        st.warning("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

    # 2. ngrok URLå…¥åŠ› (æ¯å›å¤‰ã‚ã‚‹ãŸã‚æ‰‹å‹•å…¥åŠ›)
    st.markdown("---")
    st.write("ğŸ”— **LLMæ¥ç¶š (ngrok)**")
    ngrok_url = st.text_input(
        "ngrok URL", 
        placeholder="https://xxxx.ngrok-free.app",
        disabled=not is_authenticated,
        help="ãƒ­ãƒ¼ã‚«ãƒ«PCã§ç™ºè¡Œã•ã‚ŒãŸngrokã®HTTPS URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"
    )
    
    # ãƒ¢ãƒ‡ãƒ«åã¯å›ºå®šã¾ãŸã¯å…¥åŠ›
    model_name = "local-model"

    st.markdown("---")
    st.write("ğŸ“‚ **ãƒ‡ãƒ¼ã‚¿è¨­å®š**")
    # CSVã¯ãƒªãƒã‚¸ãƒˆãƒªåŒæ¢±ã‚’åŸºæœ¬ã¨ã™ã‚‹ãŒã€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚‚è¨±å¯
    uploaded_file = st.file_uploader("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸Šæ›¸ã (ä»»æ„)", type=["csv"])
    
    # DBãƒ‘ã‚¹: Streamlit Cloudã®ãƒ‘ã‚¹æ§‹æˆã«åˆã‚ã›ã¦ç›¸å¯¾ãƒ‘ã‚¹æŒ‡å®š
    # ãƒªãƒã‚¸ãƒˆãƒªç›´ä¸‹ã« 'DB' ãƒ•ã‚©ãƒ«ãƒ€ãŒã‚ã‚‹å‰æ
    db_refs_path = st.text_input("å‚è€ƒæ–‡çŒ®DBãƒ‘ã‚¹", value="./DB")

# -----------------------------------------------------------------
# â–¼ 1. ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ‰
# -----------------------------------------------------------------
@st.cache_data
def load_data(file_or_path) -> pd.DataFrame:
    try:
        return pd.read_csv(file_or_path)
    except Exception as e:
        return pd.DataFrame()

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ­ã‚¸ãƒƒã‚¯
df_global = pd.DataFrame()

if uploaded_file is not None:
    df_global = load_data(uploaded_file)
    st.sidebar.success(f"Uploadãƒ‡ãƒ¼ã‚¿ä½¿ç”¨: {len(df_global)}ä»¶")
else:
    # ãƒªãƒã‚¸ãƒˆãƒªå†…ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆCSVã‚’æ¢ã™
    default_csv = "cases_list_integrated.csv"
    if os.path.exists(default_csv):
        df_global = load_data(default_csv)
        st.sidebar.info(f"ãƒªãƒã‚¸ãƒˆãƒªå†…ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨: {len(df_global)}ä»¶")
    else:
        st.sidebar.warning("CSVãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

# -----------------------------------------------------------------
# â–¼ 2. ãƒ„ãƒ¼ãƒ«å®šç¾©
# -----------------------------------------------------------------
# LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ç”¨ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° (ngrokãƒ˜ãƒƒãƒ€ãƒ¼å¯¾å¿œ)
def get_llm_client(base_url, api_key="lm-studio"):
    custom_client = httpx.Client(headers={"ngrok-skip-browser-warning": "true"})
    return ChatOpenAI(
        base_url=base_url,
        api_key=api_key,
        model=model_name,
        temperature=0.0,
        http_client=custom_client
    )

@tool
def get_case_details(case_id: str) -> str:
    """å˜ä¸€ã® case_id ã‚’å—ã‘å–ã‚Šã€CSVã‹ã‚‰ãã®äº‹æ¡ˆã®è©³ç´°ã‚’å–å¾—ã—ã¾ã™ã€‚"""
    if df_global.empty: return "ãƒ‡ãƒ¼ã‚¿ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
    try:
        # IDã®å‹æºã‚‰ãå¸å
        try:
            target_id = int(case_id.strip())
            record = df_global[df_global['case_id'] == target_id]
        except:
            target_id = case_id.strip()
            record = df_global[df_global['case_id'].astype(str) == target_id]

        if record.empty:
            return f"case_id '{case_id}' ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
            
        row = record.iloc[0]
        details = [f"--- case_id: {row['case_id']} ã®è©³ç´° ---"]
        for col, val in row.items():
            if pd.notna(val) and str(val).strip() != "":
                details.append(f"  - {col}: {val}")
        return "\n".join(details)
    except Exception as e:
        return f"ã‚¨ãƒ©ãƒ¼: {e}"

@tool
def analyze_statistics(query: str) -> str:
    """çµ±è¨ˆåˆ†æå°‚ç”¨ãƒ„ãƒ¼ãƒ«ã€‚Pythonã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆãƒ»å®Ÿè¡Œã—ã¦çµæœã‚’è¿”ã—ã¾ã™ã€‚"""
    # ãƒ„ãƒ¼ãƒ«å†…ã§LLMã‚’å‘¼ã¶ãŸã‚ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¾ãŸã¯å¼•æ•°ã‹ã‚‰URLå–å¾—ãŒå¿…è¦
    # ã“ã“ã§ã¯ç°¡æ˜“çš„ã«st.session_stateãªã©ã‚’å‚ç…§ã›ãšã€å†åˆæœŸåŒ–ã§å¯¾å¿œ
    if not ngrok_url:
        return "ã‚¨ãƒ©ãƒ¼: ngrok URLãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
    
    # ngrokçµŒç”±ã§Codeç”Ÿæˆç”¨LLMã‚’å‘¼ã¶
    base_url_v1 = ngrok_url.rstrip("/") + "/v1"
    coder_llm = get_llm_client(base_url_v1)

    columns_list = ", ".join(df_global.columns.tolist()) if not df_global.empty else "ãªã—"
    
    prompt = f"""
    ã‚ãªãŸã¯å„ªç§€ãªPythonãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã‚‹ãŸã‚ã® Pandas ã‚³ãƒ¼ãƒ‰ã®ã¿ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚
    å¤‰æ•°åã¯ 'df' ã‚’ä½¿ç”¨ã€‚åˆ—å: [{columns_list}]
    
    è³ªå•: {query}
    
    ãƒ«ãƒ¼ãƒ«:
    - Pythonã‚³ãƒ¼ãƒ‰ã®ã¿å‡ºåŠ›(Markdownã‚¿ã‚°ä¸è¦)
    - çµæœã¯ print() ã§å‡ºåŠ›
    - ã‚°ãƒ©ãƒ•æç”»ç¦æ­¢
    """

    try:
        response = coder_llm.invoke(prompt)
        code = response.content.replace("```python", "").replace("```", "").strip()
        
        local_env = {'df': df_global.copy(), 'pd': pd}
        old_stdout = sys.stdout
        redirected_output = io.StringIO()
        sys.stdout = redirected_output
        
        try:
            exec(code, {}, local_env)
            result = redirected_output.getvalue()
        except Exception as e:
            result = f"ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}"
        finally:
            sys.stdout = old_stdout
            
        return f"ã€åˆ†æçµæœã€‘\n{result}" if result.strip() else "çµæœãªã—"

    except Exception as e:
        return f"åˆ†æã‚¨ãƒ©ãƒ¼: {e}"

# -----------------------------------------------------------------
# â–¼ 3. RAGæ©Ÿèƒ½ (å‚è€ƒæ–‡çŒ®æ¤œç´¢)
# -----------------------------------------------------------------
@st.cache_resource(show_spinner="å‚è€ƒæ–‡çŒ®DBã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
def get_ref_retriever(db_path):
    """
    Streamlit Cloudä¸Šã§HuggingFaceãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€Chromaã‚’èª­ã¿è¾¼ã‚€ã€‚
    æ³¨æ„: åˆå›ã¯ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰(æ•°GB)ãŒèµ°ã‚‹ãŸã‚æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã€‚
    """
    try:
        if not os.path.exists(db_path):
            return None
            
        # ãƒ¢ãƒ‡ãƒ«è¨­å®š (CPUå‹•ä½œ)
        model_kwargs = {'device': 'cpu'}
        encode_kwargs = {'normalize_embeddings': False}
        embedding_model_name = "intfloat/multilingual-e5-large"
        
        embeddings = HuggingFaceEmbeddings(
            model_name=embedding_model_name,
            model_kwargs=model_kwargs,
            encode_kwargs=encode_kwargs
        )
        
        db_refs = Chroma(persist_directory=db_path, embedding_function=embeddings)
        return db_refs.as_retriever(search_kwargs={"k": 3})
    except Exception as e:
        st.error(f"DBãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def search_references_action(user_query):
    if not os.path.exists(db_refs_path):
        return "âš ï¸ ãƒªãƒã‚¸ãƒˆãƒªå†…ã« 'DB' ãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"

    retriever = get_ref_retriever(db_refs_path)
    if not retriever:
        return "DBã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
    
    try:
        docs = retriever.invoke(user_query)
        if not docs: return "é–¢é€£æ–‡çŒ®ãªã—"
        
        result_text = f"**Q: {user_query}** ã«é–¢é€£ã™ã‚‹å‚è€ƒæ–‡çŒ®:\n\n"
        for i, doc in enumerate(docs, 1):
            source = doc.metadata.get("source", "ä¸æ˜")
            content = doc.page_content.replace("\n", " ")[:300]
            result_text += f"**[{i}] {source}**\n> {content}...\n\n"
        return result_text
    except Exception as e:
        return f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}"

# -----------------------------------------------------------------
# â–¼ 4. LangGraph æ§‹ç¯‰
# -----------------------------------------------------------------
@st.cache_resource
def build_graph(_llm_client):
    # ãƒ„ãƒ¼ãƒ«ãƒªã‚¹ãƒˆ
    tools = [get_case_details, analyze_statistics]
    
    # ãƒ„ãƒ¼ãƒ«ãƒã‚¤ãƒ³ãƒ‰
    llm_with_tools = _llm_client.bind_tools(tools)

    class AgentState(TypedDict):
        messages: Annotated[list[BaseMessage], add_messages]

    def agent_node(state: AgentState):
        messages = state['messages']
        system_prompt = SystemMessage(content="""
        ã‚ãªãŸã¯æ³•å¾‹å°‚é–€å®¶ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
        ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦äº‹å®Ÿã«åŸºã¥ãå›ç­”ã—ã¦ãã ã•ã„ã€‚
        """)
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†
        if not isinstance(messages[0], SystemMessage):
            messages = [system_prompt] + messages
        
        response = llm_with_tools.invoke(messages)
        return {"messages": [response]}

    tool_node = ToolNode(tools)

    workflow = StateGraph(AgentState)
    workflow.add_node("agent", agent_node)
    workflow.add_node("tools", tool_node)
    workflow.set_entry_point("agent")
    
    def should_continue(state):
        last_message = state['messages'][-1]
        if last_message.tool_calls: return "tools"
        return END

    workflow.add_conditional_edges("agent", should_continue)
    workflow.add_edge("tools", "agent")
    
    return workflow.compile()

# -----------------------------------------------------------------
# â–¼ 5. ãƒ¡ã‚¤ãƒ³å‡¦ç†
# -----------------------------------------------------------------

# èªè¨¼ & URLãƒã‚§ãƒƒã‚¯
if not is_authenticated or not ngrok_url:
    st.info("ğŸ‘ˆ ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã¨ngrok URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# ã‚¢ãƒ—ãƒªåˆæœŸåŒ–
base_url_v1 = ngrok_url.rstrip("/") + "/v1"
main_llm_client = get_llm_client(base_url_v1)
app = build_graph(main_llm_client)

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if message.get("tool_output"):
            with st.expander("è©³ç´°ãƒ‡ãƒ¼ã‚¿"):
                st.text(message["tool_output"])

# å‚è€ƒæ–‡çŒ®ãƒœã‚¿ãƒ³
if st.session_state.messages and st.session_state.messages[-1]["role"] == "assistant":
    if len(st.session_state.messages) >= 2:
        last_user_query = st.session_state.messages[-2]["content"]
        if st.button("ğŸ“š å‚è€ƒæ–‡çŒ®ã‚‚æ¤œç´¢ã™ã‚‹"):
            with st.spinner("æ¤œç´¢ä¸­... (åˆå›ã¯ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™)"):
                ref_result = search_references_action(last_user_query)
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": ref_result,
                    "tool_output": None
                })
                st.rerun()

# ãƒãƒ£ãƒƒãƒˆå…¥åŠ›
if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›..."):
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    if df_global.empty:
        st.error("CSVãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        st.stop()

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        status_container = st.status("æ€è€ƒä¸­...", expanded=True)
        
        # LangGraphå®Ÿè¡Œç”¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å¤‰æ›
        lc_messages = []
        for m in st.session_state.messages:
            role = "user" if m["role"] == "user" else "assistant"
            lc_messages.append(HumanMessage(content=m["content"]) if role == "user" else AIMessage(content=m["content"]))

        try:
            inputs = {"messages": lc_messages}
            full_response = ""
            captured_outputs = []

            for event in app.stream(inputs, stream_mode="values"):
                last_msg = event["messages"][-1]
                
                if hasattr(last_msg, 'tool_calls') and last_msg.tool_calls:
                    for tc in last_msg.tool_calls:
                        status_container.write(f"ğŸ› ï¸ {tc['name']}")
                
                elif last_msg.type == "tool":
                    captured_outputs.append(last_msg.content)
                
                elif isinstance(last_msg, AIMessage) and not last_msg.tool_calls:
                    full_response = last_msg.content
                    message_placeholder.markdown(full_response)

            status_container.update(label="å®Œäº†", state="complete", expanded=False)
            
            if full_response:
                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": full_response,
                    "tool_output": "\n".join(captured_outputs) if captured_outputs else None
                })
                st.rerun()
                
        except Exception as e:
            status_container.update(label="ã‚¨ãƒ©ãƒ¼", state="error")
            st.error(f"é€šä¿¡ã‚¨ãƒ©ãƒ¼: {e}")
            st.info("ngrokã®URLãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")